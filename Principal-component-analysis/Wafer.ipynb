{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè≠ Detecting Manufacturing Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection is pivotal in products as it enables early issue identification, quality assurance improvement, security enhancement, performance optimization, fraud prevention, predictive maintenance, and enhanced customer experience. \n",
    "\n",
    "In this problem, we have a set of data comprising 1558 attributes from a manufactuing machine for a handful products samples and whether each product is good or anamoulous (e.g., has a defect). By training a supervised machine learning model on such data, we can be able to infer whether a new product is good or anamous given its 1558 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wafer.csv')\n",
    "# print the shape of the dataset\n",
    "print(f\"Dataset has {dataset.shape[0]} rows and {dataset.shape[1]} columns\")\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column split\n",
    "x_data = dataset.drop(columns=['Class'])     \n",
    "y_data = dataset['Class']                              \n",
    "\n",
    "# train-test split\n",
    "x_data, y_data = x_data.to_numpy(), y_data.to_numpy()\n",
    "\n",
    "# transform y_data to -1, 1 range by replacing 0s and 1s\n",
    "y_data = np.where(y_data==0, -1, 1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, \n",
    "                                                  random_state=0, stratify=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üòé Machine Learning Engineering Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our Adaboost library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Adaboost import Adaboost\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ada_model = Adaboost(T=1000)\n",
    "ada_model.fit(x_train, y_train)\n",
    "ada_y_pred = ada_model.predict(x_val)\n",
    "ada_accuracy = ada_model.score(x_val, y_val)\n",
    "print(\"AdaBoost Accuracy:\", round(ada_accuracy, 3))\n",
    "\n",
    "f1 = f1_score(y_val, ada_y_pred)\n",
    "print(\"F1 Score:\", round(f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another model to more fairly judge Adaboost's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(x_train, y_train)\n",
    "gnb_y_pred = gnb_model.predict(x_val)\n",
    "gnb_accuracy = gnb_model.score(x_val, y_val)\n",
    "\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", round(gnb_accuracy, 3))\n",
    "\n",
    "f1 = f1_score(y_val, gnb_y_pred)\n",
    "print(\"F1 Score:\", round(f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why is there a discripancy between the accuracy and F1-score for both models. In your explaination, write code that illustrates any points that need to be made and decide which of them is the right choice for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why is Gaussian Naives Bayes faster than Adaboost and why is it less performant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üë®üèª‚Äçüî¨ Now let's try PCA on the features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be so greedy. Let's start by attempting to reduce the dimensionality from 1559D to 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PCA import PCA\n",
    "\n",
    "pca = PCA(new_dim=3)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_val_pca = pca.transform(x_val)            # ask yourself, why did we not use pca.fit_transform(x_val)\n",
    "\n",
    "print(f\"Training data has {x_train_pca.shape[0]} rows and {x_train_pca.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Adaboost on the data with reduced dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Adaboost import Adaboost\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ada_model = Adaboost(T=1000)\n",
    "ada_model.fit(x_train_pca, y_train)\n",
    "ada_y_pred = ada_model.predict(x_val_pca)\n",
    "ada_accuracy = ada_model.score(x_val_pca, y_val)\n",
    "print(\"AdaBoost Accuracy:\", round(ada_accuracy, 3))\n",
    "\n",
    "f1 = f1_score(y_val, ada_y_pred)\n",
    "print(\"F1 Score:\", round(f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gaussian naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(x_train_pca, y_train)\n",
    "gnb_y_pred = gnb_model.predict(x_val_pca)\n",
    "gnb_accuracy = gnb_model.score(x_val_pca, y_val)\n",
    "\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", round(gnb_accuracy, 3))\n",
    "\n",
    "f1 = f1_score(y_val, gnb_y_pred)\n",
    "print(\"F1 Score:\", round(f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In view of the results above, explain what benefit we got from using PCA for each of Adaboost and Gaussian Naive Bayes (whether in terms of speed or performance). What effect in machine learning was responsible of making Gaussian Naive Bayes perform much worse when the dimensionality was high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" >\n",
    "    <img src=\"https://media1.tenor.com/m/pWeXVj_pjM4AAAAC/perfection-michael-fassbender.gif\" width=800>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
