{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë®üèª‚Äçüî¨ Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature transformation, we are interested in finding a function  $z=f(x)$ that transforms a given vector of featurs $(x_1, x_2,...,x_n)$ into another $(z_1, z_2,...,z_l)$ that improves the machine learning task at hand (e.g., speed or predictive performance). A common genre of feature transformation is dimensionality reduction which enforces that $l<n$ which obviously improves speed but also often improves predictive performance as alleviates the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) problem.\n",
    "\n",
    "You can easily show that if we restrict ourselves to *linear feature transformations* that is, each output feature is linear in the input features: $$z_i=f(x)=a_1 x_1 + a_2 x_2 +...+a_n x_n$$\n",
    "\n",
    "then such transformation can be represented by the following matrix multiplication for some choice of $A$:\n",
    "\n",
    "$$z_{l√ó1} = A_{l√ón} x_{n√ó1} $$\n",
    "\n",
    "and the inverse transformation could approximately reconstruct the output with:\n",
    "\n",
    "$$x_{n√ó1} = A^T_{n√ól} z_{l√ó1} $$\n",
    "\n",
    "### ‚öóÔ∏è PCA\n",
    "\n",
    "PCA solves the feature transformation problem via two steps:\n",
    "\n",
    "1. Find the matrix $A$ that transform the points into a new space where the features are uncorrelated\n",
    "\n",
    "$$z_{n√ó1} = A_{n√ón} x_{n√ó1} $$\n",
    "\n",
    "2. Filter out rows of $A$ corresponding to features of lowest variance, so the dimensionality reduction equation as expected becomes\n",
    "\n",
    "$$z_{l√ó1} = A_{l√ón} x_{n√ó1} $$\n",
    "\n",
    "‚ú¶ In the lecture you have proven that PCA can perform (1) by setting $A=U^T$ where $U_{n√ón}$ is a matrix containing (as columns) the eigenvectors $k_1, k_2, ...,k_n$ of the covariance matrix of the data which we call the *\"principal components\"* \n",
    "\n",
    "‚ú¶ You have also shown that in this case the eigenvalues of the covariance matrix $Œª_1, Œª_2,...,Œª_n$ correspond one-to-one to such eigenvectors and represent the variances of the new $n$ uncorrelated features.\n",
    "\n",
    "- Thereby, to accomplish (2), if the total variance is $Œ£_{i=0}^n Œª_i$ then we can achieve dimensionality reduction by keeping only the $l$ eigenvectors in $U$ that have most of the variance (i.e., the eigenvectors corresponding to the top $l$ eigenvalues) where $l$ is a user-defined hyperparameter.\n",
    "\n",
    "- Alternatively, we can keep adding the variances $Œª_1, Œª_2,...,Œª_n$ descendingly and stop once we have captured a percentage of the total variance (e.g., $\\frac{Œª_1+Œª_3+Œª_5}{Œ£_{i=0}^n Œª_i}=0.9$ so we only need three eigenvectors $k_1, k_3, k_5$ in $U$ to capture $90%$ of the variance in the dataset.) \n",
    "\n",
    "In either case, now $A_{l√ón}=U^T_{l√ón}$ where the columns of $U$ have been filtered to include only $l$ of the eigenvectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "### üîÑ Equivalent Transformation\n",
    "\n",
    "Notice that if $x$ and $z$ are row vectors then we rather have\n",
    "\n",
    "| Transformation Equation        | Inverse Transformation Equation |\n",
    "|-----------------------|------------------------|\n",
    "| $$z_{1√ól} = x_{1√ón} A^T_{n√ól}  $$ | $$x_{1√ón} = z_{1√ól} A_{l√ón}  $$ |\n",
    "\n",
    "\n",
    "From this, you use the defintion of matrix multiplication to show that given a matrix $X_{m,n}$ we can transform each row in the matrix to the lower dimensionality by obtaining the corresponding matrix $Z_{m,l}$ by applying:\n",
    "\n",
    "$$Z_{m√ól} = X_{m√ón} A^T_{n√ól}$$\n",
    "\n",
    "Likewise, the inverse transformation becomes\n",
    "\n",
    "$$X_{m√ón} = Z_{m√ól} A_{l√ón}  $$\n",
    "\n",
    "Observe that all we did is replace $1$ in the original equations with $m$.\n",
    "\n",
    "### üë®üèª‚Äçüíª Implementation Requirements\n",
    "\n",
    "<br><br>\n",
    "```python\n",
    "__init__(self, new_dim=None)\n",
    "```\n",
    "- Sets `new_dim` as the PCA hyperparameter $L$\n",
    "\n",
    "- Initializes PCA parameters (i.e., $A$) as well as $Œº$ and $œÉ$ for standardization\n",
    "\n",
    "<br><br>\n",
    "```python\n",
    "def fit(self, x_train):\n",
    "```\n",
    "- Standardizes the data to bring features at a similar range (which also makes covariance easier to compute but it will be externally handled anyway)\n",
    "\n",
    "- Solves the eigenvalue problem of the covariance matrix\n",
    "\n",
    "- Descendingly sorts the eigenvalues and corresponding eigenvectors ($U$)\n",
    "\n",
    "- Chooses the top $L$ eigenvectors in $U$ and sets $A=U^T$\n",
    "\n",
    "<br><br>\n",
    "```python\n",
    "def transform(self, x_val):\n",
    "```\n",
    "- Standardizes `x_val` using the stored `Œº` and `œÉ`\n",
    "- Transforms it using the PCA transformation equation\n",
    "\n",
    "<br><br>\n",
    "```python\n",
    "def inverse_transform(self, z_val)\n",
    "```\n",
    "- Transforms `z_val` using the PCA inverse transformation equation\n",
    "- Destandardize the result using the stored `Œº` and `œÉ` before returning it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr> </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PCA import PCA\n",
    "import numpy as np\n",
    "\n",
    "# generate data with zero mean and unit variance\n",
    "x_train = np.random.randn(3000000, 10)\n",
    "\n",
    "pca = PCA(new_dim=1)\n",
    "pca.fit(x_train)\n",
    "\n",
    "# assert that Œº and œÉ are zero mean and unit variance\n",
    "assert np.allclose(pca.Œº, np.zeros(10), atol=1e-2)\n",
    "assert np.allclose(pca.œÉ, np.ones(10), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as SKPCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "x_data= load_iris().data\n",
    "\n",
    "# Standardize data\n",
    "x_data = StandardScaler().fit_transform(x_data)\n",
    "\n",
    "# Define PCA models\n",
    "pca = PCA(new_dim=2)\n",
    "sk_pca =  SKPCA(n_components=2)\n",
    "pca.fit(x_data)\n",
    "sk_pca.fit(x_data)\n",
    "\n",
    "assert np.allclose(np.abs(pca.A.T), np.abs(sk_pca.components_.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_data = pca.transform(x_data)\n",
    "z_data_sk = sk_pca.transform(x_data)\n",
    "assert np.allclose(np.cov(z_data.T), np.cov(z_data_sk.T))\n",
    "assert np.allclose(np.cov(z_data.T), np.cov(z_data.T) * np.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is true that the test in the last line of the previous cell should pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "x_data= load_iris().data\n",
    "\n",
    "# Define PCA models\n",
    "pca = PCA(new_dim=4)                # full dimensionality so inverse approximation will be exact\n",
    "pca.fit(x_data)\n",
    "z_data = pca.transform(x_data)\n",
    "\n",
    "assert np.allclose(pca.inverse_transform(z_data), x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üòé  Put on Your Machine Learning Engineer Lenses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PCA import PCA  \n",
    "\n",
    "# Generate data\n",
    "np.random.seed(40)\n",
    "Œº = [0, 0]  # Mean of the distribution\n",
    "Œ£ = [[1, 0.7], [0.7, 1]]  # Covariance matrix\n",
    "\n",
    "# TODO 1: Generate multivariate normal data with the parameters above using Numpy (100 samples)\n",
    "x_data = None\n",
    "x1, x2 = x_data[:, 0], x_data[:, 1]\n",
    "y_data = ((x2 - (-1.5 * x1 + -0.1)) > 0) + 1                \n",
    "corr_line = lambda x: 0.9 * x + 0                   # correlation line\n",
    "\n",
    "# TODO 2: Perform PCA with two components \n",
    "pca = None\n",
    "z_data = pca.fit_transform(x_data)\n",
    "\n",
    "# TODO 3: Perform PCA with one component\n",
    "pca = None\n",
    "z_data_1d = pca.fit_transform(x_data)\n",
    "\n",
    "# Create subplots\n",
    "plt.style.use('dark_background')\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), dpi=200)\n",
    "\n",
    "# Plot original data and line\n",
    "axs[0].scatter(x_data[:, 0], x_data[:, 1], s=20, c=y_data, cmap='plasma', edgecolors='white', linewidths=0.3)\n",
    "axs[0].plot(x_data[:, 0], corr_line(x_data[:, 0]), color='red', linewidth=0.5)\n",
    "axs[0].set_title(\"Original Data\")\n",
    "\n",
    "# Plot transformed data by PCA with 2 components\n",
    "axs[1].scatter(z_data[:, 0], z_data[:, 1], s=20, c=y_data, cmap='plasma', edgecolors='white', linewidths=0.3)\n",
    "axs[1].set_ylim(-3, 3)\n",
    "axs[1].set_title(\"Transformed Data by PCA (2 components)\")\n",
    "\n",
    "# Plot transformed data by PCA with 1 component\n",
    "axs[2].scatter(z_data_1d, np.zeros_like(z_data_1d), s=20, c=y_data, cmap='plasma', edgecolors='white', linewidths=0.3)\n",
    "axs[2].set_title(\"Transformed Data by PCA (1 component)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions to Consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's obvious from the first figure that the data is linearly seperable, have we lost this separability after doing doing PCA in the next two plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer Goes Here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any orthogonal matrix corresponds to a specific type of transformation (read about that). Use this and the fact that U is orthogonal to explain the relationship between the points in the first and second plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer Goes Here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- In terms of projection, explain the relationship of the points in the first and third plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer Goes Here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media1.tenor.com/m/H-bzshVnNR4AAAAC/you-are-awesome-well-done.gif\" width=900>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê Extra Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The singular value decomposition of a matrix $A$ takes the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A_{m√ón} = U_{m√óm} \\Sigma_{m√ón} V_{n√ón}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It associates the matrix $A$ with:\n",
    "- $m$ left singular vectors (columns of $U$)\n",
    "- $n$ right singular vectors (columns of $V$)\n",
    "- $min(m,n)$ singular values\n",
    "\n",
    "It can be easily shown that the $n$ right singular vectors (columns of $V$) are the $n$ eigenvectors of $A^TA$ and the $m$ left singular vectors are the $m$ eigenvectors of $AA^T$. \n",
    "\n",
    "Meanwhile, the singular values are the square roots of the eigenvalues of either $A^TA$ or $AA^T$\n",
    "\n",
    "\n",
    "**Consider the following fact:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data from the multivariate normal distribution\n",
    "m = 10000\n",
    "x_data = np.random.multivariate_normal(Œº, Œ£, m)\n",
    "\n",
    "covariance_matrix = x_data.T @ x_data / (m - 1)\n",
    "\n",
    "\n",
    "assert np.allclose(np.cov(x_data, rowvar=False), covariance_matrix, rtol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, with zero-meaned (or standardized) data $X_{m√ón}$, we have $$Œ£=\\frac{X^TX}{(m-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can solve the singular value decomposition of $\\frac{X}{\\sqrt{m-1}}$ and then use its right singular vectors $A=V^T$ instead of doing eigendecompostion of $Œ£$ and then using the eigenvectors $A=U^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve eigendecomposition of the covariance matrix\n",
    "Œªs, U = np.linalg.eig(np.cov(x_data, rowvar=False))\n",
    "\n",
    "# solve singular value decomposition of X\n",
    "U_svd, S, Vt = np.linalg.svd(x_data/(m-1)**0.5)\n",
    "\n",
    "\n",
    "assert np.allclose(np.abs(U), np.abs(Vt.T), rtol=1e-2)\n",
    "assert (np.allclose(np.flip(Œªs), S**2, rtol=1e-2) or np.allclose(Œªs, S**2, rtol=1e-2))                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of doing this over eigendecomposition:\n",
    "\n",
    "- Computing the SVD from `x_train` directly is more computationally stable and possibly more efficient that computing Œ£ then eigendecomposition\n",
    "\n",
    "- The singular values in `S` will be ordered descendingly automatically (it's part of SVD definition) and they are just squares of the needed eigenvalues\n",
    "\n",
    "- Knowing more machine learning mathematics like this can be helpful for other algorithms as well (e.g., psuedo inverses and linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ú® Extra Requirements List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Revisit the implementation and use singular value decomposition instead of eigenvalue decomposition\n",
    "\n",
    "- If `new_dim` is a float between $0$ and $1$, then find the number of components $L$ such that the sum of their variances (eigenvalues) over the sum of the total variance (all eigenvalues) is at least `new_dim`. Check out `np.cumsum` for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWd6OGhvc2lqcGtwajhpZHVxaWFnNGRxeXA4b2s3cXI3aG1xNXd6NiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l49JHLpRSLhecYEmI/giphy.gif\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
