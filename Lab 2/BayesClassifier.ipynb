{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to implement the  in `BayesClassifier.py`:\n",
    "Bayes Classifier is a generic model covered in last week's lecture. It performs inference for a new point $x_{test}$ using the classification rule:\n",
    "\n",
    "$$x_{test} = argmax_{k} P\\big(C_k|x_{test}\\big) = argmax_{k} P(x_{test}|C_k) * P(C_k)$$\n",
    "\n",
    "**Three cases of the Bayes classifier were covered in the lecture each tantamount to a different machine learning model:**\n",
    "1. Assume multivariate Normal distribution for each class conditional density $P(X|C_{i})$. In this case, Bayes Classifier is also known as `Quadratic Discriminant Analysis`. This is the general setting.\n",
    "\n",
    "2. Assume Multivariate Normal distribution for each class conditional density $P(X|C_{i})$ while assuming the different classes have equal covariances $\\Sigma_1=\\Sigma_2=...=\\Sigma_K$. This is known as `Linear Discriminant Analysis`.\n",
    "    - In this case, when the different classes have unequal covariance matrices, the single one used is their weighted sum where the weights are the prior probabilities. \n",
    "    - That is for $K$ classes, $Î£ = p_1*\\Sigma_1 + p_2*\\Sigma_2 + ...+ p_K*\\Sigma_K$\n",
    "\n",
    "3. Assume one-dimensional Normal distribution for each feature in $X$, for each class. This is known as `Gaussian Naive Bayes` and it's naive in the sense that it assumes that the input features are independent when that is rarely the case. \n",
    "    - This is also equivalent to assuming that each class has a diagonal covariance matrix as a diagonal covariance matrix only occurs when the input variables (features) are uncorrelated.\n",
    "\n",
    "**You are required to implement seven methods in `BayesClassifier.py`**\n",
    "<br><br>\n",
    "```python\n",
    "__init__(self, mode='QDA')\n",
    "```\n",
    "- Decides the type of the Bayes classifier `QDA` or `LDA` or `Naive` and declares the variables for model parameters\n",
    "<br><br>\n",
    "```python\n",
    "fit(self, x_train, y_train)\n",
    "```\n",
    "- Takes two numpy arrays `x_train` and `y_train` of shapes `(m,n)` and `(m)` respectively and sets the model parameters by computing the model parameter according to the mode. It doesn't return anything; only mutates the model parameters.\n",
    "<br><br>\n",
    "```python\n",
    "N(x, Î¼, Î£)\n",
    "```\n",
    "- Given `x` of dimensions `(n,)` and `Î¼` of dimensions `(n,)` and `Î£` of dimensions `(n,n)` return a scalar probability of observing `x` using the multivariate normal distribution:\n",
    "$$P(x) = \\mathcal{N}(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} exp\\big(\\frac{-1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\big)$$\n",
    "Observe that this automatically computes $P(x|C_{k})$ when given $Î¼=Î¼_K$ and $Î£=Î£_K$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "```python\n",
    "predict_proba_x(self, x)\n",
    "```\n",
    "- Given `x` of dimensions `(n,)` use Bayes rule to compute $P(C_{k}|x)$ for $1<=k<=K$ and return such probabilities in a numpy array of dimensions `(K,)`\n",
    "- Recall that Bayes rule states:\n",
    "$$P(C_k|X)=\\frac {P(X|C_k)P(C_k)}{P(X)}$$\n",
    "\n",
    "```python\n",
    "predict_proba_x(self, x_val)\n",
    "```\n",
    "- Given `x_val` of dimensions `(m,n)` return a numpy array of dimensions `(m,K)` by applying `predict_proba_x(self, x)` to each row in `x_val`. In other words, this maps each point in the validation set to the probability of it belonging to each class.\n",
    "<br><br>\n",
    "\n",
    "```python\n",
    "predict(self, x_val)\n",
    "```\n",
    "- Given `x_val` of dimensions `(m,n)` return a numpy array of dimensions `(m)` that applies Bayes classification rule on each row in `x_val` by first computing the probabilities with `predict_proba_x` then applying argmax over each row.\n",
    "<br><br>\n",
    "\n",
    "```python\n",
    "score(self, x_val, y_val)\n",
    "```\n",
    "- Given `x_val` use Bayes classification rule to compute `y_pred` and compute the accuracy by comparing it to the true labels in `y_val`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to understand the code below. If there were more time, it would have been a task for you as well to write the testing code as in all software engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# No need to restart the notebook upon change thanks to autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from BayesClassifier import BayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_classes=3, n_clusters_per_class=1, \n",
    "                           random_state=0,  n_informative=2, n_redundant=0, class_sep=1.5, scale=10)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.style.use('dark_background')\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=20, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Testing QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Fits without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='QDA')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert my_classifier.means[0].shape == (2,)\n",
    "assert len(my_classifier.covs) == len(my_classifier.labels)\n",
    "assert my_classifier.covs[0].shape == (2,2)\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1\n",
    "assert my_classifier.weighted_cov == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Test that parameters are set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with scikit parameters\n",
    "clf = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.means, clf.means_)\n",
    "assert np.allclose(my_classifier.covs, clf.covariance_)\n",
    "assert np.allclose(my_classifier.priors, clf.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Test the PDF of the Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your implementation ###\n",
    "### DO NOT CHANGE THIS CODE ###\n",
    "np.random.seed(90)\n",
    "assertion_x = np.random.rand(3)\n",
    "assertion_mu = np.random.rand(3)\n",
    "assertion_sigma = np.random.rand(3, 3)\n",
    "assertion_probability = BayesClassifier.N(assertion_x, assertion_mu, assertion_sigma)\n",
    "assertion_probability = round(assertion_probability, 1)\n",
    "assert assertion_probability == 7.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Basic test for `predict_proba_x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict_proba_x(np.random.rand(2))\n",
    "assert y_pred.shape == (3,)\n",
    "assert np.allclose(np.sum(y_pred), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 It follows that `predict_proba` is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8 Test `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.score(x_val, y_val), clf.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Testing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='LDA')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert len(my_classifier.covs) == 0\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Test parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.weighted_cov, clf.covariance_)\n",
    "assert np.allclose(my_classifier.means, clf.means_)\n",
    "assert np.allclose(my_classifier.priors, clf.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Basic test for `predict_proba_x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict_proba_x(np.random.rand(2,))\n",
    "assert y_pred.shape == (3,)\n",
    "assert np.allclose(np.sum(y_pred), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 End-to-end tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))\n",
    "\n",
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))\n",
    "\n",
    "assert np.allclose(my_classifier.score(x_val, y_val), clf.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Testing Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='Naive')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert len(my_classifier.covs) == len(my_classifier.labels)\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1\n",
    "assert my_classifier.weighted_cov == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Test parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.priors, clf.class_prior_)\n",
    "assert np.allclose(my_classifier.means, clf.theta_)\n",
    "# loop on each row of clf.var_ and ensure the computed variances for a class are the diagonal of corresponding covariance matrix\n",
    "for i in range(clf.var_.shape[0]): assert np.allclose(my_classifier.covs[i], np.diag(clf.var_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 End-to-end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))\n",
    "\n",
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))\n",
    "\n",
    "assert np.allclose(my_classifier.score(x_val, y_val), clf.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://i.imgur.com/LMiA2O5.gif\" width=800/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
