{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”® Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the Bayes classifier in the previous lab under the normal distribution assumption of $P(X|C)$ and with as well other assumptions to result in LDA and Gaussian Naive Bayes (GNB) and their general form, QDA.\n",
    "\n",
    "Despite the fact that, thanks to the central limit theorem, many real-life variables do follow the normal distribution which makes QDA one of the most optimal machine learning algorithms you can get your hands on in such cases, many other variables will not follow the normal distribution. For instance, many variables may be categorical (discrete) in the first place or may just follow another distribution (e.g., exponential, lognormal, uniform, etc.) and sometimes we don't even know the mathematical form of the distirbution! \n",
    "\n",
    "In summary, we have three cases for what $P(x|C)$ may follow:\n",
    "\n",
    "- Normal Distribution\n",
    "\n",
    "- Some other distribution $P(x)$ with known mathematical form and algorithm to estimate its parameters\n",
    "\n",
    "- An arbitrary distribution $P(x)$ with an unkown mathematical form or parameter estimation method \n",
    "\n",
    "The third case is the most general and can be approached with density estimation. Meanwhile, the second and first cases can be more accurate than estimating the density from scratch but require more information.\n",
    "\n",
    "So far, our Bayes classifier only supports the first case and we want to extend it to support the other two:\n",
    "\n",
    "\n",
    "#### âœ¦ Generalize Bayes Classifier to Arbitrary Forms of the Distribution\n",
    "Our first goal in this lab will be to extend the Bayes Classifier implementation to handle other distributions other than the normal distribution. \n",
    "\n",
    "- The user should have the ability to pass their distribution as a function that takes two parameters, the point `x` to evaluate the probability at and a dictionary `params` containing the hyperparameters to then return the probability of the point.\n",
    "\n",
    "- The user must as well pass an `estimate_params` methods that given a set of $M$ points each of $N$ dimensions `(x_data)` returns the dictionary of parameters `params`.\n",
    "\n",
    "- Our implementation should automatically estimate the parameters for each class using the `estimate_params` function during `fit` and the parameters should be made ready when needed in a probability estimation `P`.\n",
    "\n",
    "Implement this in \n",
    "```python\n",
    "fit_custom_density(self, x_train, y_train)\n",
    "```\n",
    "and\n",
    "```python\n",
    "    def P(self, x, k)->float:\n",
    "        match self.mode:\n",
    "            ...\n",
    "            case 'Custom':\n",
    "                # implementation goes here\n",
    "        return P\n",
    "```\n",
    "\n",
    "See the test cases below for more information.\n",
    "\n",
    "#### âœ¦ Generalize Bayes Classifier to Arbitrary Distribution of Unknown Form\n",
    "\n",
    "This will make use of the `KDEstimator` class you implemented earlier. The user should be able to pass their kernel density estimator instance which will be used to estimate the density for each class `P(X|C)`\n",
    "\n",
    "Implement this in \n",
    "```python\n",
    "def fit_kde(self, x_train, y_train)\n",
    "```\n",
    "and\n",
    "```python\n",
    "def P(self, x, k)->float:\n",
    "    match self.mode:\n",
    "        ...\n",
    "        case 'KDE':\n",
    "            # implementation goes here\n",
    "    return P\n",
    "```\n",
    "\n",
    "See the test cases below for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# No need to restart the notebook upon change thanks to autoreload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from BayesClassifier import BayesClassifier\n",
    "from KDEstimator import KDEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=3000, n_features=2, n_classes=3, n_clusters_per_class=1, \n",
    "                           random_state=0,  n_informative=2, n_redundant=0, class_sep=2.5, scale=10)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.style.use('dark_background')\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=20, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Testing KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a weak test but the KDE notebook took care of stronger ones\n",
    "kde = KDEstimator(bump='Gauss', bandwidth='Scott')\n",
    "kde_clf = BayesClassifier(mode='KDE', kde_config=kde)\n",
    "kde_clf.fit(x_train, y_train)\n",
    "\n",
    "# Ensure it's equivalent to using QDA\n",
    "normal_clf = BayesClassifier(mode='QDA')\n",
    "normal_clf.fit(x_train, y_train)\n",
    "\n",
    "# Number of points is large KDE estimate should be just like real assumption (accurate)\n",
    "np.allclose(kde_clf.predict(x_val), normal_clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Testing Custom Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "# Define the density which takes x and the parameters\n",
    "def N(x, params):\n",
    "    Î¼, Î£ = params['Î¼'], params['Î£']\n",
    "    Ï€, N= np.pi, len(Î¼)\n",
    "    P = np.exp(-0.5 * ( (x - Î¼).T @ inv(Î£) @  (x- Î¼))) / np.sqrt((2*Ï€)**N * det(Î£))\n",
    "    return P\n",
    "\n",
    "# Define a function that estimates the paramers of the density\n",
    "def estimate_params(x_data):\n",
    "    Î¼ = np.mean(x_data, axis=0)\n",
    "    Î£ = np.cov(x_data, rowvar=False)\n",
    "    return {'Î¼': Î¼, 'Î£':Î£}\n",
    "\n",
    "# Consider passing a custom density which is normal distribution in this case\n",
    "# Normal usecace will certainly be an external distribution but this is just to test\n",
    "custom_clf = BayesClassifier(mode='Custom', density=N, estimate_params=estimate_params)\n",
    "custom_clf.fit(x_train, y_train)\n",
    "\n",
    "# Ensure it's equivalent to using QDA\n",
    "normal_clf = BayesClassifier(mode='QDA')\n",
    "normal_clf.fit(x_train, y_train)\n",
    "\n",
    "# Assert equivalence\n",
    "np.allclose(custom_clf.predict_proba(x_val), normal_clf.predict_proba(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Kernel Density & Custom Density Combo Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should pass if the two tests above pass. This uses `KDE` as a custom density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(x, params):\n",
    "    kde = params['kde']\n",
    "    return kde.transform(x)\n",
    "\n",
    "# define a function that estimates the paramers of the density\n",
    "def estimate_params(x_data):\n",
    "    kde = KDEstimator(bandwidth='Scott', bump='Gauss')\n",
    "    kde.fit(x_data)\n",
    "    return {'kde': kde}\n",
    "\n",
    "custom_clf = BayesClassifier(mode='Custom', density=P, estimate_params=estimate_params)\n",
    "custom_clf.fit(x_train, y_train)\n",
    "\n",
    "np.all(kde_clf.predict(x_val) == custom_clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following tests are just in case you decide to modify any of the existing Bayes classifier code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Testing QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Fits without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='QDA')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert my_classifier.means[0].shape == (2,)\n",
    "assert len(my_classifier.covs) == len(my_classifier.labels)\n",
    "assert my_classifier.covs[0].shape == (2,2)\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Test that parameters are set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with scikit parameters\n",
    "clf = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.means, clf.means_)\n",
    "assert np.allclose(my_classifier.covs, clf.covariance_)\n",
    "assert np.allclose(my_classifier.priors, clf.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 Basic test for `predict_proba_x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict_proba_x(np.random.rand(2))\n",
    "assert y_pred.shape == (3,)\n",
    "assert np.allclose(np.sum(y_pred), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 It follows that `predict_proba` is implemented correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7 Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8 Test `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.score(x_val, y_val), clf.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Testing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='LDA')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert not hasattr(my_classifier, 'covs'), \"LDA should not have covs attribute\"\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Test parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='eigen', store_covariance=True)\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.weighted_cov, clf.covariance_)\n",
    "assert np.allclose(my_classifier.means, clf.means_)\n",
    "assert np.allclose(my_classifier.priors, clf.priors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Basic test for `predict_proba_x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_classifier.predict_proba_x(np.random.rand(2,))\n",
    "assert y_pred.shape == (3,)\n",
    "assert np.allclose(np.sum(y_pred), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 End-to-end tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))\n",
    "\n",
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Testing Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = BayesClassifier(mode='GNB')\n",
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Test basic properties of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(my_classifier.means) == len(my_classifier.labels)\n",
    "assert len(my_classifier.covs) == len(my_classifier.labels)\n",
    "assert len(my_classifier.priors) == len(my_classifier.labels)\n",
    "assert my_classifier.priors.sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Test parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(x_train, y_train)\n",
    "assert np.allclose(my_classifier.priors, clf.class_prior_)\n",
    "assert np.allclose(my_classifier.means, clf.theta_)\n",
    "# loop on each row of clf.var_ and ensure the computed variances for a class are the diagonal of corresponding covariance matrix\n",
    "for i in range(clf.var_.shape[0]): assert np.allclose(my_classifier.covs[i], np.diag(clf.var_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 End-to-end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_classifier.predict_proba(x_val), clf.predict_proba(x_val))\n",
    "\n",
    "y_pred = my_classifier.predict(x_val)\n",
    "assert y_pred.shape == y_val.shape\n",
    "assert np.allclose(y_pred, clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://i.imgur.com/LMiA2O5.gif\" width=800/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
