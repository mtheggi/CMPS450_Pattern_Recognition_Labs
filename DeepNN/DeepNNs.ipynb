{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Single Neuron Recap}$ \n",
    "\n",
    "In the previous tutorial we have seen how to implement a *single neuron* neural network for both of regression and classification.\n",
    "\n",
    "For instance, in case of binary classification:\n",
    "\n",
    "<font color=\"pink\"> 1. **Define the hypothesis function**</font>  as the logistic function applied to the linear combination of input features and parameters:\n",
    "   $$ yÃÇ = f(x; \\theta) = f(w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n) = \\sigma(w^T u) \\quad \\text{where} \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "      \n",
    "<font color=\"pink\"> 2. **Define the loss function**</font>  as the cross-entropy loss function:\n",
    "   $$J(\\theta) = -\\frac{1}{M} \\sum_{m=1}^{M} \\left[ y_m \\log(yÃÇ_m) + (1 - y_m) \\log(1 - yÃÇ_m) \\right] $$\n",
    "\n",
    "<font color=\"pink\"> 3. **Train the model** </font> by iteratively applying gradient descent:\n",
    "$$ w_{t+1} = w_t - \\alpha \\frac{\\partial J(w_t)}{\\partial w} $$\n",
    "\n",
    "- Recall that $J(w)$ (and it's derivative) are considered functions of the weight vector $w$ only because the data computed in the loss function is constant. \n",
    "\n",
    "- Also recall that we had to use a numerical method such as gradient descent because it was infeasible to set $\\frac{\\partial J(w_t)}{\\partial w} = 0$; we were only aple to compute $\\frac{\\partial J(w_t)}{\\partial w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the same scheme can be used for regression except that $f(z)=z$ (i.e., no function is applied on the linear combination) and the loss function measures an actual difference. E.g.,\n",
    "$$J(w) = Œ£_{m=1}^{M} (w^t u_m -y_m)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with such models is that they are linear when the vast majority of datasets are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neuron import LogisticRegression               # from last tutorial\n",
    "from sklearn.datasets import  make_circles\n",
    "from Plot import plot_model_contours\n",
    "\n",
    "x_data, y_data = make_circles(n_samples=1200, random_state=42, factor=0.3, noise=0.12)\n",
    "\n",
    "# Fit-Predict Pipeline\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_data, y_data)\n",
    "plot_model_contours(lr, x_data, y_data, trained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"aqua\"> \n",
    "\n",
    "$\\text{Extending to Deep Networks}$ \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have covered in the lecture, for a neural network:\n",
    "\n",
    "<font color=\"pink\"> **1. Define the hypothesis function**: </font>\n",
    "\n",
    "- In a neural network, the hypothesis function is a sequence of $L$ <font color=\"orange\"> weight matrix multiplications </font> (as defined below) that sequentially transforms the $n$ dimensional input $x$ to $n_1,n_2,...,n_L$ dimensions respectively (i.e., $n_L$ is the final output dimension) and where each matrix multiplication is followed by a <font color=\"orange\">nonlinear activation function </font> (i.e., $f_1, f_2, ..., f_L$) respectively, that applies element-wise on the output vector.\n",
    "\n",
    "That is,\n",
    "$$\n",
    "x\n",
    "\\xrightarrow{f_1(W^{(1)}x+b^{(1)})} \n",
    "Y^{(1)}\n",
    "\\xrightarrow{f_2(W^{(2)}Y^{(1)}+b^{(2)})}\n",
    "Y^{(2)}\n",
    "\\xrightarrow{\\dots}\n",
    "Y^{(L-1)}\n",
    "\\xrightarrow{f_L(W^{(L)}Y^{(L-1)}+b^{(L)})}\n",
    "Y^{(L)}\n",
    "$$\n",
    "Which we can write more verbosely as:\n",
    "$$\n",
    "x_{(n,1)}\n",
    "\\xrightarrow{f_1(W^{(1)}_{(n_1,n)}x_{(n,1)}+b^{(1)}_{(n_1,1)})}\n",
    "Y^{(1)}_{(n_1,1)}\n",
    "\\xrightarrow{f_2(W^{(2)}_{(n_2,n_1)}Y^{(1)}_{(n_1,1)}+b^{(2)}_{(n_2,1)})}\n",
    "Y^{(2)}_{(n_2,1)}\n",
    "\\xrightarrow{\\dots}\n",
    "Y^{(L-1)}_{(n_{L-1},1)}\n",
    "\\xrightarrow{f_L(W^{(L)}_{(n_L,n_{L-1})}Y^{(L-1)}_{(n_{L-1},1)}+b^{(L)}_{(n_L,1)})}\n",
    "Y^{(L)}_{(n_L,1)}\n",
    "$$\n",
    "\n",
    "Note that we usually add bias explictly than have it in $u$ to avoid needing to append $1$ to each layer.\n",
    "<details>\n",
    "<summary><b>Example:</b></summary>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://i.imgur.com/CUdktNK.png\" width=700>\n",
    "</div>\n",
    "\n",
    "For instance, for this neural network we have $L=3$ where $(n_1, n_2, n_3) = (4, 4, 1)$ and we can write its expression as:\n",
    "\n",
    "- We don't count the input layer in $L$ because it only provides the input $u$ of dimensionality $n$ and does not correspond to a matrix multiplcation. Ideally the plot shouldn't make it look like other layers but it's widely used by people.\n",
    "\n",
    "- In this network:\n",
    "<div align=\"center\">\n",
    "\n",
    "\n",
    "| Operation at L=1 | Operation at L=2 | Operation at L=3 |\n",
    "|----------|----------|----------|\n",
    "| I.e., \"Hidden Layer 1\" | I.e., \"Hidden Layer 2\" | I.e., \"Output Layer\" |\n",
    "| $Y_{(4,1)}^{(1)} = f_1(W_{(4,3)}x_{(3,1)}+b^{(1)}_{(4,1)})$ | $Y_{(4,1)}^{(2)} = f_2(W_{(4,4)}Y^{(1)}_{(4,1)}+b^{(2)}_{(4,1)}$ | $Y_{(1,1)}^{(3)} = f_3(W_{(1,4)}Y^{(2)}_{(4,1)}+b^{(3)}_{(1,1)})$ |\n",
    "\n",
    "</div>\n",
    "\n",
    "- Should be clear that every layer of index $l$ (except the input layer) corresponds to a weight matrix of dimensions $W_{(n_l,n_{l-1})}$ and a bias vector of dimensions $b_{n_l}$\n",
    "\n",
    "- Note that the plot itself does not define $f_1, f_2, f_3$ which are part of the mathematical (i.e., actual) definition\n",
    "\n",
    "- As an exercise, assume $f_3$ is the sigmoid function and that $f_1$ and $f_2$ are the ReLU functions and try to write the hypothesis function in one line on paper.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Conclusions:</b></summary>\n",
    "\n",
    "\n",
    "- To define the neural network (i.e., its hypothesis function):\n",
    "\n",
    "   - We need to choose as a hyperparameter the number of layers $L$ (we call the first $L-1$ of them hidden layers and the last, the output layer.)\n",
    "\n",
    "   - We need to choose the number of neurons per layer (i.e., $n_1$, $n_2$,...,$n_L$). \n",
    "   \n",
    "      - In case of binary classification and regression tasks our prediction can be a single probability/value and we set $n_L=1$ but we will see other cases later\n",
    "\n",
    "   - We need to choose the activation functions $f_1, f_2, ..., f_L$. Typically can use the same function for all of them except for the output layer (e.g., in binary classification we need to get a probility so we must use Sigmoid)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Rationale:</b></summary>\n",
    "<br>\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <td>\n",
    "\n",
    "Our problem with the single neuron model we have seen above is that it's linear. As machine learning engineers, we know that casting the data into a new space where the data becomes approximately linear can solve the nonlinearity problem.<br><br>That is, given an input feature vector (while training or inference) $x_1,x_2,...,x_n$ we can cast it into a $p$-dimensional space and perform classification there:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3 \\\\\n",
    "    ... \\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "\\xrightarrow{z=f(x)}\n",
    "\\begin{bmatrix}\n",
    "    z_1 \\\\\n",
    "    z_2 \\\\\n",
    "    z_3 \\\\\n",
    "    ... \\\\\n",
    "    z_{p}\n",
    "\\end{bmatrix}\n",
    "\\xrightarrow{classification}\n",
    "f(w_0 + w_1z_1 + w_2z_2 + \\ldots + w_nz_n) = \\sigma(w^T u)\n",
    "$$\n",
    "\n",
    "Many classical machine learning models use this idea already as we have seen (kernels). However, they treat the transformation as a constant which hinders the ability of obtaining adequate fits of the data (i.e., it's easy to show that great transformations are data-dependent).\n",
    "\n",
    "In a neural network, a layer $l$ does a weight matrix multiplication followed by a nonlinear activation (i.e., $f_l(W Y^{(l-1)}+b)$) which is exactly a nonlinear transformation defined by $W$ and $f$ that transforms the input vector $Y^{(l-1)}$ of dimensionality $n_{(l-1)}$ into another $Y^{(L)}$ of dimensionality $n_l$ via the weight matrix $W_{(n_l,n_{l-1})}$ (and the bias vector $b_{n_{l-1}}$ to allow shifting).\n",
    "\n",
    "These transformations are the basis of deep learning and they outperform the classical machine learning transformations because they are learnable (i.e., by training on the data sets $W$ which defines the transformation) and because stacking them results in more and more complexity which helps when tackling hard problems (e.g., image and audio).\n",
    "</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Back to logistic regression, you can easily show that it acts like a single neuron of a neural network because:\n",
    "\n",
    "$$ Y^{(l)} =\n",
    "\\begin{bmatrix}\n",
    "    Y^{(l)}_1 \\\\\n",
    "    Y^{(l)}_2 \\\\\n",
    "    Y^{(l)}_3 \\\\\n",
    "    ... \\\\\n",
    "    Y^{(l)}_{n_l}\n",
    "\\end{bmatrix}\n",
    "= f_l(W Y^{(l-1)} + b) =\n",
    "\\begin{bmatrix}\n",
    "    f_l(w_1 ‚ãÖ Y^{(l-1)} + b_1) \\\\\n",
    "    f_l(w_2 ‚ãÖ Y^{(l-1)} + b_2) \\\\\n",
    "    f_l(w_3 ‚ãÖ Y^{(l-1)} + b_3) \\\\\n",
    "    ... \\\\\n",
    "    f_l(w_{n_{l}} ‚ãÖ Y^{(l-1)} + b_{n_{l}})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "That is, the output of each neuron in the $l$ layer performs the same operation as logistic regression if $f_l$ is the sigmoid function. In this our notation assumes $w_1, w_2, ..., w_{n_l}$ to be the rows of $W^l$ and $b_1, b_2, ..., b_{n_l}$ to be the elements of the bias vector $b^{(l)}$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\"> **2. Define the loss function**: </font>\n",
    "\n",
    "- As we known this depends on the task where the two most popular options are squared loss and cross-entropy loss:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Regression                          | Classification                                                                                 |\n",
    "|-------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| $ J(\\theta) = \\frac{1}{2M} \\sum_{m=1}^{M} (y_m - \\hat{y}_m)^2$         | $ J(\\theta) = -\\frac{1}{M} \\sum_{m=1}^{M} \\left[ y_m \\log(\\hat{y}_m) + (1 - y_m) \\log(1 - \\hat{y}_m) \\right]$ |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In our case, $yÃÇ_m$ is $Y^L$ (i.e., the output from the output layer when the network is given $x_m$) and $y_m$ is the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, we can also use any other loss function, so long as minimizing it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\"> 3. **Train the model** </font> by iteratively applying gradient descent (or any other optimizer):\n",
    "$$ w_{t+1} = w_t - \\alpha \\frac{\\partial J(w_t)}{\\partial w} $$ \n",
    "for every single weight matrix or bias vector in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should be aware that setting $\\frac{\\partial J(w_t)}{\\partial w} = 0$ is certainly unfeasible because the fact that $Y^L$ is a complex expression straightforwardly implies that $J$ and it's derivative also are (which is why we must use an optimziation algorithm like GD).\n",
    "\n",
    "  - In fact, deriving the expression of $\\frac{\\partial J(w_t)}{\\partial w}$ is nontrivial; it requires a whole algorithm called Backpropagation\n",
    "\n",
    "- As we know, to compute $J$ or it's derivative we need to compute the output of the network for all the training examples ${(x_1,y_1), (x_2, y_2),...,(x_m,y_m)}$. However, it can be sometimes be too big for our VRAM to hold this amount of data and it may not have the best optimization properties.\n",
    "\n",
    "  - As you (likely) covered in the lecture, we still converge, if in each iteration, we only input some random sample (i.e., batch) of the data instead of the whole data and in this case, the algorithm is called mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ü§î Rationale:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to what we mentioned in the rationale section above, neural networks are fantastic machine learning models because:\n",
    "\n",
    "- Unlike most classical machine learning models they don't make assumptions on the data (e.g., linearity, normality, correlation, etc.)\n",
    "\n",
    "\n",
    "- Unlike classical machine learning models, there is no specific bound to the model's complexity (i.e., adding more nerons or more layers will always increase complexity)\n",
    "\n",
    "   - Hence, they are used for unstructured data which usually is too complex to fit (e.g., images, text, audio, etc.)\n",
    "\n",
    "   - It can be visually seen that the transformations that the layers learn sometimes correspond to actual meaningful features we can understand (hence, we mentioned before they automate feature extraction)\n",
    "\n",
    "- This scalability property makes them, unlike classical machine learning models, able to make use of of big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's verify their ability to fit nonlinear data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following uses a neural network implemented via PyTorch in `Torch.py`. Don't worry we will delve into that with detail after the next tutorial and you will implement your own neural network from scratch in this lab. This is only to showcase the ability of neural networks and ensure that PyTorch works on your machine.\n",
    "\n",
    "You must be able to run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Torch import ClassificationNN, plot_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ClassificationNN()\n",
    "model.fit(x_data, y_data, epochs=250)\n",
    "plot_model_contours(model, x_data, y_data, trained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Universal Approximation Theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, a neural network, with at least one hidden layer, is a universal approximator any continuous function (or three or more hidden layers for discontinuous functions).\n",
    "\n",
    "That is, whatever is the function mapping $x$ to $y$ in your dataset, there exists a neural network with at least three hidden layers and some number of neurons that can fit it to any accuracy. In other words, there are \"no bounds\" to how nonlinear (i.e., complex) your model can get. Hence, NNs success in text, image and audio data (most interesting!).\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>See an instance of the theorem:</b></summary>\n",
    "<img src=\"https://i.imgur.com/sCujL12.png\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that out ourselves. We have designed a very complex binary classifcation dataset in `dataset.csv`; let's see how complex it is by training the model on it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must be able to run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load and normalize the dataset\n",
    "dataset = df = pd.read_csv('dataset.csv').values\n",
    "x_data, y_data = dataset[:, :2], dataset[:, 2:]    \n",
    "       \n",
    "Œº = np.mean(x_data, axis=0)\n",
    "œÉ = np.std(x_data, axis=0)\n",
    "x_data = (x_data - Œº) / œÉ\n",
    "\n",
    "# Define and train your model\n",
    "model = ClassificationNN()\n",
    "model.fit(x_data, y_data, epochs=1000)           # An ideal setting is perhaps epochs=2500. Try not to go much lower than 1000 so it converges\n",
    "\n",
    "# Evaluate model on whole domain to see what function is approximated:\n",
    "plot_classification(model, x_data, Œº, œÉ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Quite complex data, eh?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
