{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚¨ÖÔ∏è Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main goal for this lab is to have you implement a neural network from scratch. The core of your work will be implementing the backpropagation algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given is a network with $L$ layers we have $L$ weight matrices $(W^{(1)},W^{(2)},...,W^{(L)})$ and $L$ bias vectors $(b^{(1)},b^{(2)},...,b^{(L)})$ as we have shown earlier\n",
    "\n",
    "- The goal of backpropagation is to find $\\frac{\\partial J}{\\partial W}$ and $\\frac{\\partial J}{\\partial b} $ for all the weights and biases in the network; this way we can train our neural network using gradient descent as demonstrated earlier.\n",
    "\n",
    "In the following we write the final expressions an algorithm for backpropagation for cross-entropy loss. Unlike the lecture we will use $A^{(l)}$ instead of $Y^{(l)}$ for the output vector of layer $l$ and use $Z^{(l)}$ instead of $X^{(l)}$ for the outputs of layer $l$ before the activation (i.e., $Z^{(l)} = WY^{(l-1)}+b^l$)\n",
    "\n",
    "- It trivially holds for any weight matrix $W$ (or bias vector $b$):\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W} = \\sum_{m} \\frac{\\partial J_m}{‚àÇ W} \\;\\;  \\text{where} \\;\\; J_m \\;\\; \\text{is the loss function for one point} \\;\\; (x_m,y_m)$$\n",
    "\n",
    "- For $W^{(L)}$ and $b^{(L)}$ (i.e., last weight matrix/bias vector only) we have:\n",
    "\n",
    "<font color=\"aqua\">\n",
    "\n",
    "$$\\frac{\\partial J_m}{\\partial Z^{(L)}} = (A^{(L)} - y_m)  $$\n",
    "\n",
    "</font>\n",
    "\n",
    "$$\\frac{\\partial J_m}{\\partial W^{(L)}} = \\frac{\\partial J_m}{\\partial Z^{(L)}}  (A^{(L-1)})^T \\quad \\text{and} \\quad \\frac{\\partial J_m}{\\partial b^{(L)}} = \\frac{\\partial J_m}{\\partial Z^{(L)}}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- Meanwhile, for any arbitrary weight matrix $W^{(l)}$ and bias vector $b^{(l)}$ we have:\n",
    "\n",
    "<font color=\"aqua\">\n",
    "\n",
    "$$\\frac{\\partial J_m}{\\partial Z^{(l)}} = ((W^{(l+1)})^T \\frac{\\partial J_m}{\\partial Z^{(l+1)}} ) * h'(Z^{(l)})  $$\n",
    "\n",
    "</font>\n",
    "\n",
    "$$\\frac{\\partial J_m}{\\partial W^{(l)}} = \\frac{\\partial J_m}{\\partial Z^{(l)}} \\; (A^{(l-1)})^T \\quad \\text{and} \\quad \\frac{\\partial J_m}{\\partial b^{(l)}} =  \\frac{\\partial J_m}{\\partial Z^{(l)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It doesn't require black magic to derive these (just prerequisites, logic and following the lecture).\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose we define $Œ¥^{(L)} = \\frac{\\partial J_m}{\\partial Z^{(l)}}$ then we can rewrite the equations above as:\n",
    "\n",
    "<font color=\"aqua\">\n",
    "\n",
    "$$Œ¥^{(l)} = (A^{(L)} - y_m) \\quad \\text{if}\\quad l==L \\quad{else}\\quad (W^{(l+1)})^T Œ¥^{(l+1)} * h'(Z^{(l)})  $$\n",
    "\n",
    "</font>\n",
    "\n",
    "$$\\frac{\\partial J_m}{\\partial W^{(l)}} = Œ¥^{(l)} (A^{(l-1)})^T \\quad \\text{and} \\quad \\frac{\\partial J_m}{\\partial b^{(l)}} = Œ¥^{(l)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where for $l=1$ (i.e., first layer) we use the input (i.e., $x$) as the previous layer activations $A^{(l-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it becomes obvious that the backpropagation algorithm to compute $\\frac{\\partial J_m}{\\partial W^{(l)}} \\;\\; \\text{and } \\;\\; \\frac{\\partial J_m}{\\partial b^{(l)}}$ for each layer $l$ given a single point $(x_m, y_m)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\"> \n",
    "\n",
    "$\\text{Backpropagation Algorithm}:$  \n",
    "\n",
    "</font>\n",
    "\n",
    "$\\text{1. Compute} \\; Z^{(l)} \\; \\text{and} \\; A^{(l)} \\text{for each layer } \\; l‚àà\\{1,2,...,L\\} \\; \\text{by passing the point onto the network}$\n",
    "\n",
    "$\\text{2. Compute} \\; Œ¥^{(L)} = (A^{(L)} - y_m) \\; \\text{and then} \\; \\frac{\\partial J_m}{\\partial W^{(L)}} = Œ¥^{(L)} (A^{(L-1)})^T \\;\\; \\text{and} \\;\\; \\frac{\\partial J_m}{\\partial b^{(L)}} =  Œ¥^{(L)} \\; \\text{for the last layer}$\n",
    "\n",
    "$\\text{3. Compute} \\; Œ¥^{(l)} = ((W^{(l+1)})^T Œ¥^{(l+1)}) * h'(Z^{(l)}) \\; \\text{and then} \\; \\frac{\\partial J_m}{\\partial W^{(l)}} = Œ¥^{(l)} (A^{(l-1)})^T \\;\\; \\text{and} \\;\\; \\frac{\\partial J_m}{\\partial b^{(l)}} =  Œ¥^{(l)} \\; \\text{for the remaining layers} \\; l‚àà\\{L-1, ..., 2, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"pink\"> Note that: </font>\n",
    "\n",
    "- The first step where we compute $Z^{(l)}$ and $A^{(l)}$ for each layer $l$ as we will need them for the derivatives is called the forward pass.\n",
    "\n",
    "- The second and third step where we computed the needed derivatives are called the backward pass. It must go backward because $Œ¥^{(l)}$ depends on $Œ¥^{(l+1)}$ except for the base case of $Œ¥^{(L)}$\n",
    "  - This is also why the algorithm is called \"backpropagation\".\n",
    "\n",
    "- The fourth step, exterior to the algorithm, is to pass the derivatives to gradient descent so that it can perform it's weight update given the point(s) $(x_m, y_m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now go to `Backprop.py` to implement an arbitrary neural network from scratch (including backpropagation) after reading the following:\n",
    "\n",
    "```python\n",
    "def __init__(self, structure, activation='relu', random_state=42)\n",
    "```\n",
    "\n",
    "- This function is responsible for making the neural network given the `structure`. For instance, if the user passes for the structure `[3, 4, 5, 1]` then our network has four layers with 3, 4, 5, 2 neurons respectively. We will leave it to the user to make sure that `3` is indeed the size of their input (i.e., number of features) and that `1` is the size of their output.\n",
    "\n",
    "- The `activation` denotes the activation function of each layer except the last one (will be always sigmoid since this is a binary classification network)\n",
    "\n",
    "- This constructor will as well randomly initialize all the weights and biases in the network\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def feedforward(self, x, store_outputs=False):\n",
    "```\n",
    "- Pass the input `x` to the network and returns the output of the network (i.e., output of the last layer)\n",
    "\n",
    "- Optionally, it can store the `Z` and `A` for each layer because they will be needed if this is called for the sake of backpropagation\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def backprop(self, x‚Çò , y‚Çò):\n",
    "```\n",
    "- Applies the backpropagation algorithm given a point as described above\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def SGD(self, x_batch, y_batch, Œ±):\n",
    "```\n",
    "- Takes a random sample of data `(x_batch, y_batch)` and learning rate `Œ±` then gets the derivative given each point in that sample\n",
    "\n",
    "- Then applies gradient descent over this random sample using its derivative to update the weight matrix and bias vector of all layers in the network\n",
    "\n",
    "- To make sense of this, recall:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W} = \\sum_{m} \\frac{\\partial J_m}{‚àÇ W} \\;\\;  \\text{where} \\;\\; J_m \\;\\; \\text{is the loss function for one point} \\;\\; (x_m,y_m)$$\n",
    "\n",
    "<br>            \n",
    "\n",
    "```python\n",
    "def fit(self, x_train, y_train, batch_size=32, epochs=200, Œ±=0.03, eval_train=False):\n",
    "```\n",
    "\n",
    "- Given training data it splits it into random samples  of data (batches) of size `batch_size` then applies gradient descent with learning rate `Œ±` for `epochs` iterations. \n",
    "\n",
    "- Optionally, it can evaluate the model on the training data after each epoch.\n",
    "\n",
    "- UPDATE: These training hyperparameters have been moved to `init` for easier access from the plot function\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def predict(self, x_val):\n",
    "```\n",
    "- Predict the label of each point in `x_val` by getting the network output of each (probability) then rounding it\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def score(self, x_val, y_val):\n",
    "```\n",
    "- Call `predict` to get `y_pred` then compute accuracy by comparing with `y_val`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Backprop import ClassificationNeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some light tests that can help you during the implementation. Feel free to extend them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_init():\n",
    "    structure = [2, 3, 1]  # Example structure\n",
    "    net = ClassificationNeuralNet(structure)\n",
    "\n",
    "    # Check if number of layers is correct\n",
    "    assert len(net.W‚Çô) == len(structure) - 1\n",
    "    assert len(net.B‚Çô) == len(structure) - 1\n",
    "\n",
    "    # Check shapes of weight matrices and bias vectors\n",
    "    for i in range(len(net.W‚Çô)):\n",
    "        W = net.W‚Çô[i]\n",
    "        B = net.B‚Çô[i]\n",
    "        n_l, n_l_prev = structure[i+1], structure[i]\n",
    "        \n",
    "        assert W.shape == (n_l, n_l_prev)\n",
    "        assert B.shape == (n_l, 1)\n",
    "\n",
    "test_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feedforward():\n",
    "    structure = [2, 3, 4]  # Example structure\n",
    "    net = ClassificationNeuralNet(structure)\n",
    "\n",
    "    x = np.random.rand(2, 1)  # Example input\n",
    "\n",
    "    # Perform forward pass\n",
    "    yÃÇ, Z‚Çô, A‚Çô = net.feedforward(x, store_outputs=True)\n",
    "\n",
    "    # Check output shape\n",
    "    assert yÃÇ.shape == (4, 1)\n",
    "\n",
    "    # Check shapes of stored outputs\n",
    "    assert len(Z‚Çô) == len(net.W‚Çô)\n",
    "    assert len(A‚Çô) == len(net.W‚Çô)\n",
    "    \n",
    "test_feedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_backprop():\n",
    "    structure = [2, 3, 1]  # Example structure\n",
    "    net = ClassificationNeuralNet(structure)\n",
    "\n",
    "    x = np.random.rand(2, 1)  # Example input\n",
    "    y = np.random.rand(1, 1)  # Example output\n",
    "\n",
    "    # Perform backpropagation\n",
    "    ·ÉõB‚Çô, ·ÉõW‚Çô = net.backprop(x, y)\n",
    "\n",
    "    # Ensure derivative exists for every single parameter\n",
    "    assert len(·ÉõB‚Çô) == len(net.B‚Çô)\n",
    "    assert len(·ÉõW‚Çô) == len(net.W‚Çô)\n",
    "    for ·ÉõB, ·ÉõW, n_l, n_l_prev in zip(·ÉõB‚Çô, ·ÉõW‚Çô, structure[1:], \n",
    "                                       structure[:-1]):\n",
    "        assert ·ÉõB.shape == (n_l, 1)\n",
    "        assert ·ÉõW.shape == (n_l, n_l_prev)\n",
    "\n",
    "test_backprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Ultimate Test:** your model should accuracy 1.0 (or too close) and have a great nonlienar fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import  make_moons\n",
    "\n",
    "model = ClassificationNeuralNet([2, 100, 1], activation='relu', eval_train=True, )\n",
    "x_data, y_data = make_moons(n_samples=2000, random_state=42, noise=0.1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "plot_model_contours(model, x_train, y_train, trained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üòé Machine Learning Goggles in Place!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks outperform classical machine learning models in many facets, the sheer number of hyperparameters makes them harder to tune. \n",
    "\n",
    "#### We have to choose the following structure hyperparameters:\n",
    "\n",
    "- Number of neurons per each layer\n",
    "\n",
    "- Number of layers\n",
    "\n",
    "- Activation function for hidden layers\n",
    "\n",
    "#### As well as the following optimization hyperparameters\n",
    "\n",
    "- Optimization algorithm (here we use gradient descent)\n",
    "\n",
    "- Loss function (here we use cross-entropy loss for classification)\n",
    "\n",
    "- Learningr rate Œ±\n",
    "\n",
    "- Batch size (random sample size for gradient descent)\n",
    "\n",
    "- Number of epochs (number of gradient descent iterations over dataset)\n",
    "\n",
    "... and even more that you will see in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠ê Analyzing Number of Neurons per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "x_data, y_data = make_moons(n_samples=2000, random_state=42, noise=0.15)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "hyperparameter_list = [\n",
    "    {\"structure\": [2, 5, 1]},\n",
    "    {\"structure\": [2, 10, 1]},\n",
    "    {\"structure\": [2, 50, 1]},\n",
    "    {\"structure\": [2, 100, 1]},\n",
    "    {\"structure\": [2, 500, 1]},\n",
    "    {\"structure\": [2, 1000, 1]},\n",
    "    {\"structure\": [2, 5000, 1]},\n",
    "    {\"structure\": [2, 10000, 1]},\n",
    "]\n",
    "\n",
    "plot_model_contours(ClassificationNeuralNet, x_train, y_train, hyperparams_list=hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, adding more neurons to a layer makes the approximation to the original function better by adding more nonlinear elements that combine together at the same order of nonlinearity. However, too many neurons can make us overfit by approximating noise as well.\n",
    "\n",
    "```python\n",
    "In light of this, explain what happened above as we increased the number of neurons in the hidden layer. Which number if the most optimal?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠ê Analyzing the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "hyperparameter_list = [\n",
    "    {\"structure\": [2, 20, 1], \"epochs\": 50},\n",
    "    {\"structure\": [2, 20, 10, 1], \"epochs\": 50,},\n",
    "    {\"structure\": [2, 20, 10, 1], \"epochs\": 50, \"Œ±\": 0.005},\n",
    "    {\"structure\": [2, 20, 30, 20, 1], \"epochs\": 50, \"Œ±\": 0.0008},\n",
    "]\n",
    "\n",
    "model = ClassificationNeuralNet(structure=[2, 30, 2, 1], epochs=50)\n",
    "\n",
    "plot_model_contours(ClassificationNeuralNet, x_train, y_train, hyperparams_list=hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, adding more layers makes the approximation to the original function better by increasing the order of nonlinearity: for instance, with one hidden layer, the basic unit used to approximate the function is a simple curve that's weightedly combined to approximate the function; meanwhile, by adding a layer we make the basic unit used in the approximation become even more complex, so it can attain overall more complex fits.\n",
    "\n",
    " However, too many neurons can make us overfit by approximating noise as well.\n",
    "\n",
    "```python\n",
    "In light of this, explain what happened above as we increased the number of hidden layers. Which number if the most optimal? Suppose we are modelling very complex data such as images or audio then do you think it would be better to add more neurons or more layers?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "In light of the experiment above, does changing a structure hyperparameter such as the number of hidden layers affect convergence properties? Why do you think a lower learning is is better when using more layers?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠ê Analyzing Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "hyperparameter_list = [\n",
    "    {\"structure\": [2, 100, 1], \"activation\": \"sigmoid\", \"epochs\": 20},\n",
    "    {\"structure\": [2, 100, 1], \"activation\": \"sigmoid\", \"epochs\": 120},\n",
    "    {\"structure\": [2, 100, 1], \"activation\": \"relu\", \"epochs\": 20},\n",
    "]\n",
    "\n",
    "plot_model_contours(ClassificationNeuralNet, x_train, y_train, hyperparams_list=hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "State one reason from the lecture justifying why ReLU yields faster convergence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠ê Analyzing Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "hyperparameter_list = [\n",
    "    {\"structure\": [2, 100, 1], \"Œ±\": 0.002},\n",
    "    {\"structure\": [2, 100, 1], \"Œ±\": 0.002, \"epochs\": 100},\n",
    "    {\"structure\": [2, 100, 1],  \"Œ±\": 0.02},\n",
    "    {\"structure\": [2, 100, 1],  \"Œ±\": 0.2},\n",
    "    {\"structure\": [2, 100, 1],  \"Œ±\": 2},\n",
    "]\n",
    "\n",
    "plot_model_contours(ClassificationNeuralNet, x_train, y_train, hyperparams_list=hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Explain why each of 0.002, 0.2 and 2 are not ideal learning rates.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠ê Analyzing Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Plot import plot_model_contours\n",
    "\n",
    "hyperparameter_list = [\n",
    "    {\"structure\": [2, 100, 1], \"batch_size\": 1},\n",
    "    {\"structure\": [2, 100, 1], \"batch_size\": 100},\n",
    "    {\"structure\": [2, 100, 1],  \"batch_size\": 300},\n",
    "    {\"structure\": [2, 100, 1],  \"batch_size\": 600},\n",
    "    {\"structure\": [2, 100, 1],  \"batch_size\": 1000,}\n",
    "]\n",
    "\n",
    "plot_model_contours(ClassificationNeuralNet, x_train, y_train, hyperparams_list=hyperparameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "The network converges to a pooerer minimum in the same number of epochs for larger batch size. Read the abstract of this paper and justify: \n",
    "```\n",
    "[Paper Link](https://arxiv.org/abs/1609.04836)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Answer goes here\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
